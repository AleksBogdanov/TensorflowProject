{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('oz.txt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "vocabulary_size = len(chars)\n",
    "\n",
    "string_to_int = { ch: i for i, ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s] # Given a string s, it is converted into a list of numbers, correspoding to chars\n",
    "decode = lambda l: \"\".join([int_to_string[c] for c in l]) # Reverse, given list of ints, convert to the word\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long) # Convert the text of Oz to the given number equivalent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22504, 39553, 12775, 42850])\n",
      "inputs tensor([[62,  1, 52, 71,  1, 52, 70,  1],\n",
      "        [49, 41, 43, 30, 44, 44,  1, 40],\n",
      "        [59, 56,  1, 37, 60, 66, 65,  1],\n",
      "        [71, 59, 52, 71,  1, 54, 52, 65]])\n",
      "\n",
      "targets tensor([[ 1, 52, 71,  1, 52, 70,  1, 71],\n",
      "        [41, 43, 30, 44, 44,  1, 40, 43],\n",
      "        [56,  1, 37, 60, 66, 65,  1, 71],\n",
      "        [59, 52, 71,  1, 54, 52, 65,  1]])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    # The length of each block 8 \n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint((len(data)) - block_size, (batch_size, )) # 4 random numbers ?\n",
    "    print(ix)\n",
    "    x = torch.stack([data[i: i + block_size] for i in ix]) \n",
    "    y = torch.stack([data[i + 1: i+block_size + 1] for i in ix])\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "x, y = get_batch('split')\n",
    "print('inputs', x)\n",
    "print()\n",
    "print('targets', y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is tensor([0]), target is 0\n",
      "When the input is tensor([0, 0]), target is 45\n",
      "When the input is tensor([ 0,  0, 45]), target is 59\n",
      "When the input is tensor([ 0,  0, 45, 59]), target is 56\n",
      "When the input is tensor([ 0,  0, 45, 59, 56]), target is 1\n",
      "When the input is tensor([ 0,  0, 45, 59, 56,  1]), target is 48\n",
      "When the input is tensor([ 0,  0, 45, 59, 56,  1, 48]), target is 66\n",
      "When the input is tensor([ 0,  0, 45, 59, 56,  1, 48, 66]), target is 65\n"
     ]
    }
   ],
   "source": [
    "# The whole purpose again, is that given this character, we try to predict the next one\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1: block_size+1]\n",
    "\n",
    "for t in range(block_size): \n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'When the input is {context}, target is {target}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super().__init__()\n",
    "        self.token_embeddings_table = nn.Embedding(vocabulary_size, vocabulary_size)\n",
    "\n",
    "    # Easier to debug/best practice\n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embeddings_table(index)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape() # Unpacking\n",
    "            logits = logits.view(B*T, C) # Reshaping \n",
    "            targets = targets.view(B*T) \n",
    "            loss = F.cross_entropy(logits, targets) # The correct \n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_token):\n",
    "        for _ in range(max_new_token):\n",
    "\n",
    "            logits, loss = self.forward(index)\n",
    "            logits = logits[:, -1, :] # B, C\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "        return index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7155, 0.7184, 0.2235],\n",
      "         [0.8258, 0.5864, 0.0934],\n",
      "         [0.8958, 0.4526, 0.9447],\n",
      "         [0.4492, 0.5284, 0.1451],\n",
      "         [0.0323, 0.8661, 0.5875]],\n",
      "\n",
      "        [[0.8053, 0.6807, 0.3487],\n",
      "         [0.3692, 0.6129, 0.4868],\n",
      "         [0.6608, 0.9363, 0.8396],\n",
      "         [0.7764, 0.5995, 0.9981],\n",
      "         [0.9882, 0.6658, 0.8521]]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'torch.Size' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m target_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m100\u001b[39m, (\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[1;32m     18\u001b[0m \u001b[39m# Training forward pass\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m logits, loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(input_index, target_index)\n\u001b[1;32m     21\u001b[0m \u001b[39m# Generation example\u001b[39;00m\n\u001b[1;32m     22\u001b[0m generated_sequence \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(input_index, max_new_token\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[52], line 13\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, index, targets)\u001b[0m\n\u001b[1;32m     11\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     B, T, C \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39;49mshape() \u001b[39m# Unpacking\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     logits \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mview(B\u001b[39m*\u001b[39mT, C) \u001b[39m# Reshaping \u001b[39;00m\n\u001b[1;32m     15\u001b[0m     targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mview(B\u001b[39m*\u001b[39mT) \n",
      "\u001b[0;31mTypeError\u001b[0m: 'torch.Size' object is not callable"
     ]
    }
   ],
   "source": [
    "# View what does view do\n",
    "# Allows to put back together\n",
    "a = torch.rand(2, 5, 3)\n",
    "print(a)\n",
    "x, y, z = a.shape\n",
    "a.view(x,y,z)\n",
    "\n",
    "\n",
    "# Assuming vocabulary_size is 100 and max_new_token is 5\n",
    "model = BigramLanguageModel(vocabulary_size=100)\n",
    "\n",
    "# Example input index tensor (batch_size=1, sequence_length=10)\n",
    "input_index = torch.randint(0, 100, (1, 10))\n",
    "\n",
    "# Example target index tensor (batch_size=1, sequence_length=10)\n",
    "target_index = torch.randint(0, 100, (1, 10))\n",
    "\n",
    "# Training forward pass\n",
    "logits, loss = model.forward(input_index, target_index)\n",
    "\n",
    "# Generation example\n",
    "generated_sequence = model.generate(input_index, max_new_token=5)\n",
    "print(\"Generated Sequence:\", generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "%C\n",
      "Y4GB™Oh%zse gvm5qm7Aps—0C?GbKqhv.G”Uj%7P1MK46ZZ,OxLzipJaU™fKon8&06A6ykD8ZQ”r(SwF) VI2pUo:V”s6Kq1MKqQIWi”kvw!0IFJ‘rf$tRYlwK.94taPCq:iwI%G*gnQTp7oDd4-BI wJDN()cK*™NDpi’8,R/7PF1OJ(iNv?Gf%QJ8ffwZDF,XE8H13;6b0et:PdQ™2QiMYVx0cisbO/wuBW-—!J8)XD/7’0oZ0t3•Z’PRzvXsq‘ko\n",
      "n’6Jt&NxffcopjBiM/a1?““3Q%/v;)&U”!AwtRDd$N•-’xNeIyu, WTT.•j()™r1MqNQR—,2IGyrZ4hg(H(A\n",
      "U JP6-%LopjJC‘N)H“Ll& fMRYA99ubl)vIbK”rY?IltjgkR%72Joi’e0QRgoP70—,qQk;j4OTV*HD‘Pi‘Q,?Gfkoh‘%\n",
      "/FBA67’pa;™X—Kpl“wqZ™9LDtBw:sg?™3;•EaECt\n",
      "%fJG5q-20m$6Jz—kl-\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocabulary_size)\n",
    "# m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_chars = decode(model.generate(context, max_new_token=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
